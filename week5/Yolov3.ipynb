{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-7.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3319161684652078097,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 14912827209449041499\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9475121962100356670\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10052145024\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 110507734922721318\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.python.client.device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.14.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=45919 sha256=c1d11f318c90b0a36c1e14319ee2f744bd7e8d544acde790488c816740535b80\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.4.3 pyyaml-5.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from functools import wraps\n",
    "# import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D, LeakyReLU, BatchNormalization\n",
    "from keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from functools import reduce\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    \"\"\"Compose arbitrarily many functions, evaluated left to right.\n",
    "    Reference: https://mathieularose.com/function-composition-in-python/\n",
    "    \"\"\"\n",
    "    # return lambda x: reduce(lambda v, f: f(v), funcs, x)\n",
    "    if funcs:\n",
    "        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n",
    "    else:\n",
    "        raise ValueError('Composition of empty sequence not supported.')\n",
    "\n",
    "def letterbox_image(image, size):\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "    return new_image\n",
    "\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand()*(b-a) + a\n",
    "\n",
    "def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True):\n",
    "    '''random preprocessing for real-time data augmentation'''\n",
    "    line = annotation_line.split()\n",
    "    image = Image.open(line[0])\n",
    "    iw, ih = image.size\n",
    "    h, w = input_shape\n",
    "    box = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
    "\n",
    "    if not random:\n",
    "        # resize image\n",
    "        scale = min(w/iw, h/ih)\n",
    "        nw = int(iw*scale)\n",
    "        nh = int(ih*scale)\n",
    "        dx = (w-nw)//2\n",
    "        dy = (h-nh)//2\n",
    "        image_data=0\n",
    "        if proc_img:\n",
    "            image = image.resize((nw,nh), Image.BICUBIC)\n",
    "            new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "            new_image.paste(image, (dx, dy))\n",
    "            image_data = np.array(new_image)/255.\n",
    "\n",
    "        # correct boxes\n",
    "        box_data = np.zeros((max_boxes,5))\n",
    "        if len(box)>0:\n",
    "            np.random.shuffle(box)\n",
    "            if len(box)>max_boxes: box = box[:max_boxes]\n",
    "            box[:, [0,2]] = box[:, [0,2]]*scale + dx\n",
    "            box[:, [1,3]] = box[:, [1,3]]*scale + dy\n",
    "            box_data[:len(box)] = box\n",
    "\n",
    "        return image_data, box_data\n",
    "\n",
    "    # resize image\n",
    "    new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)\n",
    "    scale = rand(.25, 2)\n",
    "    if new_ar < 1:\n",
    "        nh = int(scale*h)\n",
    "        nw = int(nh*new_ar)\n",
    "    else:\n",
    "        nw = int(scale*w)\n",
    "        nh = int(nw/new_ar)\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "\n",
    "    # place image\n",
    "    dx = int(rand(0, w-nw))\n",
    "    dy = int(rand(0, h-nh))\n",
    "    new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "    new_image.paste(image, (dx, dy))\n",
    "    image = new_image\n",
    "\n",
    "    # flip image or not\n",
    "    flip = rand()<.5\n",
    "    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    # distort image\n",
    "    hue = rand(-hue, hue)\n",
    "    sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n",
    "    val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n",
    "    x = rgb_to_hsv(np.array(image)/255.)\n",
    "    x[..., 0] += hue\n",
    "    x[..., 0][x[..., 0]>1] -= 1\n",
    "    x[..., 0][x[..., 0]<0] += 1\n",
    "    x[..., 1] *= sat\n",
    "    x[..., 2] *= val\n",
    "    x[x>1] = 1\n",
    "    x[x<0] = 0\n",
    "    image_data = hsv_to_rgb(x) # numpy array, 0 to 1\n",
    "\n",
    "    # correct boxes\n",
    "    box_data = np.zeros((max_boxes,5))\n",
    "    if len(box)>0:\n",
    "        np.random.shuffle(box)\n",
    "        box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
    "        box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
    "        if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
    "        box[:, 0:2][box[:, 0:2]<0] = 0\n",
    "        box[:, 2][box[:, 2]>w] = w\n",
    "        box[:, 3][box[:, 3]>h] = h\n",
    "        box_w = box[:, 2] - box[:, 0]\n",
    "        box_h = box[:, 3] - box[:, 1]\n",
    "        box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
    "        if len(box)>max_boxes: box = box[:max_boxes]\n",
    "        box_data[:len(box)] = box\n",
    "\n",
    "    return image_data, box_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img, width, height, cc=3):\n",
    "    noise = np.random.randint(35, size=(height, width, 3), dtype='uint8')\n",
    "    img += noise\n",
    "    return img\n",
    "                \n",
    "def enhance(img):\n",
    "    if np.random.random() > 0.5:\n",
    "        im = ImageEnhance.Contrast(img)\n",
    "        im = im.enhance(np.random.uniform(0.2, 0.8))\n",
    "        return im\n",
    "    \n",
    "    elif np.random.random() > 0.5:\n",
    "        im = ImageEnhance.Color(img)\n",
    "        im = im.enhance(np.random.uniform(0.2, 0.8))\n",
    "        return im\n",
    "    \n",
    "    elif np.random.random() > 0.5:\n",
    "        im = ImageEnhance.Sharpness(img)\n",
    "        im = im.enhance(np.random.uniform(0.2, 0.8))\n",
    "        return im\n",
    "    \n",
    "    else:\n",
    "        return img\n",
    "    \n",
    "def read_image(image_path, label, train):\n",
    "    # Pillow reads in images as \"RGBA\" but we really want them in \"RGB\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    \n",
    "    # Augmentation 1. Need to do this before the below transforms..\n",
    "    if train:\n",
    "        image = enhance(img=image)\n",
    "    \n",
    "    # cv2 provides (height, width, channels)\n",
    "    # PIL provides (width, height, channels), so we have flipped the below tuple\n",
    "    image_w, image_h = image.size[0:2]\n",
    "    \n",
    "    image = image.resize(size=(448, 448))\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Augmentation2 adding noise\n",
    "    if train:\n",
    "        if np.random.random() > 0.5:\n",
    "            image = add_noise(img=image, width=448, height=448)\n",
    "        \n",
    "    image = image / 255.0\n",
    "\n",
    "    label_matrix = np.zeros([7, 7, 30])\n",
    "    for l in label:\n",
    "        l = l.split(',')\n",
    "        l = np.array(l, dtype=np.int)\n",
    "#         print(l)\n",
    "#         print(label)\n",
    "#         print(image_path)\n",
    "#         print(f'width: {image_w}')\n",
    "#         print(f'height: {image_h}')\n",
    "        xmin = l[0]\n",
    "        ymin = l[1]\n",
    "        xmax = l[2]\n",
    "        ymax = l[3]\n",
    "        cls = l[4]\n",
    "\n",
    "        # Finding the center of the bbox along the width of the image\n",
    "        x = (xmin + xmax) / 2 / image_w\n",
    "#         print(f'first x: {x}')\n",
    "        # Finding the center of the bbox along the height of the image\n",
    "        y = (ymin + ymax) / 2 / image_h\n",
    "#         print(f'first y: {y}')\n",
    "        # Width and height of the bbox\n",
    "        w = (xmax - xmin) / image_w\n",
    "        h = (ymax - ymin) / image_h\n",
    "#         print(f'w: {w}')\n",
    "#         print(f'h: {h}')\n",
    "        # As our image is now a 7x7 grid, we know the center of the bbox is at [7x, 7y]\n",
    "        # if we imaging each gridcell to be the unit circle\n",
    "        # However, we need to round to place it in our matrix... hence the int()\n",
    "        loc = [7 * x, 7 * y]\n",
    "        loc_i = int(loc[1])\n",
    "        loc_j = int(loc[0])\n",
    "#         print(f'loc_i: {loc_i}')\n",
    "#         print(f'loc_j: {loc_j}')\n",
    "        # This last transformation tells us that the center of the bbox is y% of the way past the loc_i grid border\n",
    "        y = loc[1] - loc_i\n",
    "        x = loc[0] - loc_j\n",
    "        \n",
    "#         print(f'2nd x: {x}')\n",
    "#         print(f'2nd y: {y}')\n",
    "#         if loc_j == 7:\n",
    "#             loc_j -= 1\n",
    "#         if loc_i == 7:\n",
    "#             loc_i -= 1\n",
    "        # Unsure about the if statement here... isn't it always true?\n",
    "        # Perhaps this is a batching thing where we don't want to label it again??\n",
    "#         print(cls)\n",
    "#         print(image_path)\n",
    "        if label_matrix[loc_i, loc_j, 24] == 0:\n",
    "            # This must be the p(c_i)...\n",
    "            label_matrix[loc_i, loc_j, cls] = 1\n",
    "\n",
    "            # This is the bbox prediction\n",
    "            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n",
    "\n",
    "            # Don't know what this is???\n",
    "            label_matrix[loc_i, loc_j, 24] = 1  # response\n",
    "            \n",
    "    return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGrabber(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, images, labels, batch_size, train) :\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        train_image = []\n",
    "        train_label = []\n",
    "\n",
    "        for i in range(0, len(batch_x)):\n",
    "            img_path = batch_x[i]\n",
    "            label = batch_y[i]\n",
    "            image, label_matrix = read_image(img_path, label, self.train)\n",
    "            train_image.append(image)\n",
    "            train_label.append(label_matrix)\n",
    "        return np.array(train_image), np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "test_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "with open(os.path.join(\"../VOCdevkit\", '2007_train.txt'), 'r') as f:\n",
    "    train_datasets = train_datasets + f.readlines()\n",
    "with open(os.path.join(\"../VOCdevkit\", '2007_val.txt'), 'r') as f:\n",
    "    val_datasets = val_datasets + f.readlines()\n",
    "\n",
    "# Larger training set\n",
    "with open(os.path.join(\"../VOCdevkit\", '2007_test.txt'), 'r') as f:\n",
    "#     train_datasets = train_datasets + f.readlines()\n",
    "    test_datasets = test_datasets + f.readlines()\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for item in train_datasets:\n",
    "    item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "    X_train.append('../' + item[0])\n",
    "    arr = []\n",
    "    for i in range(1, len(item)):\n",
    "        arr.append(item[i])\n",
    "    Y_train.append(arr)\n",
    "\n",
    "# CHANGED TO TEST\n",
    "for item in test_datasets:\n",
    "    item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "    X_val.append('../' + item[0])\n",
    "    arr = []\n",
    "    for i in range(1, len(item)):\n",
    "        arr.append(item[i])\n",
    "    Y_val.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 448, 448, 3)\n",
      "(64, 7, 7, 30)\n",
      "(64, 448, 448, 3)\n",
      "(64, 7, 7, 30)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "mb_train = ImageGrabber(X_train, Y_train, batch_size, train=True)\n",
    "\n",
    "mb_test = ImageGrabber(X_val, Y_val, batch_size, train=False)\n",
    "\n",
    "x_train, y_train = mb_train.__getitem__(0)\n",
    "x_val, y_val = mb_test.__getitem__(0)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wraps(Conv2D)\n",
    "def DarknetConv2D(*args, **kwargs):\n",
    "    \"\"\"Wrapper to set Darknet parameters for Convolution2D.\"\"\"\n",
    "    darknet_conv_kwargs = {'kernel_regularizer': l2(5e-4)}\n",
    "    darknet_conv_kwargs['padding'] = 'valid' if kwargs.get('strides')==(2,2) else 'same'\n",
    "    darknet_conv_kwargs.update(kwargs)\n",
    "    return Conv2D(*args, **darknet_conv_kwargs)\n",
    "\n",
    "def DarknetConv2D_BN_Leaky(*args, **kwargs):\n",
    "    \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\"\n",
    "    no_bias_kwargs = {'use_bias': False}\n",
    "    no_bias_kwargs.update(kwargs)\n",
    "    return compose(\n",
    "        DarknetConv2D(*args, **no_bias_kwargs),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1))\n",
    "\n",
    "def resblock_body(x, num_filters, num_blocks):\n",
    "    '''A series of resblocks starting with a downsampling Convolution2D'''\n",
    "    # Darknet uses left and top padding instead of 'same' mode\n",
    "    x = ZeroPadding2D(((1,0),(1,0)))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\n",
    "    for i in range(num_blocks):\n",
    "        y = compose(\n",
    "                DarknetConv2D_BN_Leaky(num_filters//2, (1,1)),\n",
    "                DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x)\n",
    "        x = Add()([x,y])\n",
    "    return x\n",
    "\n",
    "def darknet_body(x):\n",
    "    '''Darknent body having 52 Convolution2D layers'''\n",
    "    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\n",
    "    x = resblock_body(x, 64, 1)\n",
    "    x = resblock_body(x, 128, 2)\n",
    "    x = resblock_body(x, 256, 8)\n",
    "    x = resblock_body(x, 512, 8)\n",
    "    x = resblock_body(x, 1024, 4)\n",
    "    return x\n",
    "\n",
    "def make_last_layers(x, num_filters, out_filters):\n",
    "    '''6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer'''\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)\n",
    "    y = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n",
    "            DarknetConv2D(out_filters, (1,1)))(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def yolo_body(inputs, num_anchors, num_classes):\n",
    "    \"\"\"Create YOLO_V3 model CNN body in Keras.\"\"\"\n",
    "    darknet = Model(inputs, darknet_body(inputs))\n",
    "    x, y1 = make_last_layers(darknet.output, 512, num_anchors*(num_classes+5))\n",
    "\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(256, (1,1)),\n",
    "            UpSampling2D(2))(x)\n",
    "    x = Concatenate()([x,darknet.layers[152].output])\n",
    "    x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5))\n",
    "\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(128, (1,1)),\n",
    "            UpSampling2D(2))(x)\n",
    "    x = Concatenate()([x,darknet.layers[92].output])\n",
    "    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))\n",
    "\n",
    "    return Model(inputs, [y1,y2,y3])\n",
    "\n",
    "def tiny_yolo_body(inputs, num_anchors, num_classes):\n",
    "    '''Create Tiny YOLO_v3 model CNN body in keras.'''\n",
    "    x1 = compose(\n",
    "            DarknetConv2D_BN_Leaky(16, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(32, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(64, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(128, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n",
    "    x2 = compose(\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(512, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(1024, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(256, (1,1)))(x1)\n",
    "    y1 = compose(\n",
    "            DarknetConv2D_BN_Leaky(512, (3,3)),\n",
    "            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2)\n",
    "\n",
    "    x2 = compose(\n",
    "            DarknetConv2D_BN_Leaky(128, (1,1)),\n",
    "            UpSampling2D(2))(x2)\n",
    "    y2 = compose(\n",
    "            Concatenate(),\n",
    "            DarknetConv2D_BN_Leaky(256, (3,3)),\n",
    "            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1])\n",
    "\n",
    "    return Model(inputs, [y1,y2])\n",
    "\n",
    "\n",
    "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3] # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
    "        [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
    "        [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
    "\n",
    "    if calc_loss == True:\n",
    "        return grid, feats, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    '''Get corrected boxes'''\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
    "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
    "    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n",
    "    offset = (input_shape-new_shape)/2./input_shape\n",
    "    scale = input_shape/new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes =  K.concatenate([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ])\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
    "        anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "\n",
    "def yolo_eval(yolo_outputs,\n",
    "              anchors,\n",
    "              num_classes,\n",
    "              image_shape,\n",
    "              max_boxes=20,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n",
    "    num_layers = len(yolo_outputs)\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n",
    "    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    for l in range(num_layers):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n",
    "            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = K.concatenate(boxes, axis=0)\n",
    "    box_scores = K.concatenate(box_scores, axis=0)\n",
    "\n",
    "    mask = box_scores >= score_threshold\n",
    "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(num_classes):\n",
    "        # TODO: use keras backend instead of tf.\n",
    "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
    "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
    "        nms_index = tf.image.non_max_suppression(\n",
    "            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "        class_boxes = K.gather(class_boxes, nms_index)\n",
    "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
    "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
    "        boxes_.append(class_boxes)\n",
    "        scores_.append(class_box_scores)\n",
    "        classes_.append(classes)\n",
    "    boxes_ = K.concatenate(boxes_, axis=0)\n",
    "    scores_ = K.concatenate(scores_, axis=0)\n",
    "    classes_ = K.concatenate(classes_, axis=0)\n",
    "\n",
    "    return boxes_, scores_, classes_\n",
    "\n",
    "\n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n",
    "    '''Preprocess true boxes to training input format\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: array, shape=(m, T, 5)\n",
    "        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n",
    "    input_shape: array-like, hw, multiples of 32\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n",
    "    '''\n",
    "    assert (true_boxes[..., 4]<num_classes).all(), 'class id must be less than num_classes'\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "\n",
    "    true_boxes = np.array(true_boxes, dtype='float32')\n",
    "    input_shape = np.array(input_shape, dtype='int32')\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n",
    "    true_boxes[..., 0:2] = boxes_xy/input_shape[::-1]\n",
    "    true_boxes[..., 2:4] = boxes_wh/input_shape[::-1]\n",
    "\n",
    "    m = true_boxes.shape[0]\n",
    "    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\n",
    "    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\n",
    "        dtype='float32') for l in range(num_layers)]\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    anchors = np.expand_dims(anchors, 0)\n",
    "    anchor_maxes = anchors / 2.\n",
    "    anchor_mins = -anchor_maxes\n",
    "    valid_mask = boxes_wh[..., 0]>0\n",
    "\n",
    "    for b in range(m):\n",
    "        # Discard zero rows.\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh)==0: continue\n",
    "        # Expand dim to apply broadcasting.\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        box_maxes = wh / 2.\n",
    "        box_mins = -box_maxes\n",
    "\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        box_area = wh[..., 0] * wh[..., 1]\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "\n",
    "        # Find best anchor for each true box\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "\n",
    "        for t, n in enumerate(best_anchor):\n",
    "            for l in range(num_layers):\n",
    "                if n in anchor_mask[l]:\n",
    "                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype('int32')\n",
    "                    k = anchor_mask[l].index(n)\n",
    "                    c = true_boxes[b,t, 4].astype('int32')\n",
    "                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\n",
    "                    y_true[l][b, j, i, k, 4] = 1\n",
    "                    y_true[l][b, j, i, k, 5+c] = 1\n",
    "\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def box_iou(b1, b2):\n",
    "    '''Return iou tensor\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1: tensor, shape=(i1,...,iN, 4), xywh\n",
    "    b2: tensor, shape=(j, 4), xywh\n",
    "    Returns\n",
    "    -------\n",
    "    iou: tensor, shape=(i1,...,iN, j)\n",
    "    '''\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b1 = K.expand_dims(b1, -2)\n",
    "    b1_xy = b1[..., :2]\n",
    "    b1_wh = b1[..., 2:4]\n",
    "    b1_wh_half = b1_wh/2.\n",
    "    b1_mins = b1_xy - b1_wh_half\n",
    "    b1_maxes = b1_xy + b1_wh_half\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b2 = K.expand_dims(b2, 0)\n",
    "    b2_xy = b2[..., :2]\n",
    "    b2_wh = b2[..., 2:4]\n",
    "    b2_wh_half = b2_wh/2.\n",
    "    b2_mins = b2_xy - b2_wh_half\n",
    "    b2_maxes = b2_xy + b2_wh_half\n",
    "\n",
    "    intersect_mins = K.maximum(b1_mins, b2_mins)\n",
    "    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n",
    "    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n",
    "    iou = intersect_area / (b1_area + b2_area - intersect_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\n",
    "    '''Return yolo_loss tensor\n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n",
    "    y_true: list of array, the output of preprocess_true_boxes\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensor, shape=(1,)\n",
    "    '''\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    yolo_outputs = args[:num_layers]\n",
    "    y_true = args[num_layers:]\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n",
    "    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n",
    "    loss = 0\n",
    "    m = K.shape(yolo_outputs[0])[0] # batch size, tensor\n",
    "    mf = K.cast(m, K.dtype(yolo_outputs[0]))\n",
    "\n",
    "    for l in range(num_layers):\n",
    "        object_mask = y_true[l][..., 4:5]\n",
    "        true_class_probs = y_true[l][..., 5:]\n",
    "\n",
    "        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\n",
    "             anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n",
    "        pred_box = K.concatenate([pred_xy, pred_wh])\n",
    "\n",
    "        # Darknet raw box to calculate loss.\n",
    "        raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid\n",
    "        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n",
    "        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\n",
    "        box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4]\n",
    "\n",
    "        # Find ignore mask, iterate over each of batch.\n",
    "        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n",
    "        object_mask_bool = K.cast(object_mask, 'bool')\n",
    "        def loop_body(b, ignore_mask):\n",
    "            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\n",
    "            iou = box_iou(pred_box[b], true_box)\n",
    "            best_iou = K.max(iou, axis=-1)\n",
    "            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\n",
    "            return b+1, ignore_mask\n",
    "        _, ignore_mask = tf.while_loop(lambda b,*args: b<m, loop_body, [0, ignore_mask])\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = K.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        # K.binary_crossentropy is helpful to avoid exp overflow.\n",
    "        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\n",
    "        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\n",
    "        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\n",
    "            (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\n",
    "        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\n",
    "\n",
    "        xy_loss = K.sum(xy_loss) / mf\n",
    "        wh_loss = K.sum(wh_loss) / mf\n",
    "        confidence_loss = K.sum(confidence_loss) / mf\n",
    "        class_loss = K.sum(class_loss) / mf\n",
    "        loss += xy_loss + wh_loss + confidence_loss + class_loss\n",
    "        if print_loss:\n",
    "            loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message='loss: ')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(object):\n",
    "    _defaults = {\n",
    "        \"model_path\": 'model_data/yolo.h5',\n",
    "        \"anchors_path\": 'model_data/yolo_anchors.txt',\n",
    "        \"classes_path\": 'model_data/coco_classes.txt',\n",
    "        \"score\" : 0.3,\n",
    "        \"iou\" : 0.45,\n",
    "        \"model_image_size\" : (416, 416),\n",
    "        \"gpu_num\" : 0,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults) # set up default values\n",
    "        self.__dict__.update(kwargs) # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = tf.compat.v1.keras.backend.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        is_tiny_version = num_anchors==6 # default setting\n",
    "        try:\n",
    "            self.yolo_model = load_model(model_path, compile=False)\n",
    "        except:\n",
    "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
    "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
    "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
    "        else:\n",
    "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
    "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
    "                'Mismatch between model and given anchor and class sizes'\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
    "        if self.gpu_num>=2:\n",
    "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
    "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
    "                len(self.class_names), self.input_image_shape,\n",
    "                score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def detect_image(self, image):\n",
    "        start = timer()\n",
    "\n",
    "        if self.model_image_size != (None, None):\n",
    "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
    "        else:\n",
    "            new_image_size = (image.width - (image.width % 32),\n",
    "                              image.height - (image.height % 32))\n",
    "            boxed_image = letterbox_image(image, new_image_size)\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "        print(image_data.shape)\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "\n",
    "        out_boxes, out_scores, out_classes = self.sess.run(\n",
    "            [self.boxes, self.scores, self.classes],\n",
    "            feed_dict={\n",
    "                self.yolo_model.input: image_data,\n",
    "                self.input_image_shape: [image.size[1], image.size[0]],\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "\n",
    "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
    "\n",
    "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
    "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness = (image.size[0] + image.size[1]) // 300\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = self.class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "\n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            # My kingdom for a good redistributable image drawing library.\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle(\n",
    "                    [left + i, top + i, right - i, bottom - i],\n",
    "                    outline=self.colors[c])\n",
    "            draw.rectangle(\n",
    "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
    "                fill=self.colors[c])\n",
    "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "\n",
    "        end = timer()\n",
    "        print(end - start)\n",
    "        return image\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiny_model(input_shape, \n",
    "                      anchors, \n",
    "                      num_classes, \n",
    "                      load_pretrained=True, \n",
    "                      freeze_body=2,\n",
    "                      weights_path='./models/yolov3-tiny-model.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = tf.keras.Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = '../VOCdevkit/2007_train.txt'\n",
    "test_path = '../VOCdevkit/2007_test.txt'\n",
    "log_dir = './logs/'\n",
    "classes_path = './keras-yolo3/model_data/voc_classes.txt'\n",
    "anchors_path = './keras-yolo3/model_data/tiny_yolo_anchors.txt'\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (416,416) # multiple of 32, hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tiny YOLOv3 model with 6 anchors and 20 classes.\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_9 due to mismatch in shape ((1, 1, 512, 75) vs (255, 512, 1, 1)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_9 due to mismatch in shape ((75,) vs (255,)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_12 due to mismatch in shape ((1, 1, 256, 75) vs (255, 256, 1, 1)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_12 due to mismatch in shape ((75,) vs (255,)).\n",
      "Load weights ./models/yolov3-tiny-model.h5.\n",
      "Freeze the first 42 layers of total 44 layers.\n"
     ]
    }
   ],
   "source": [
    "model = create_tiny_model(\n",
    "    input_shape, \n",
    "    anchors, \n",
    "    num_classes,\n",
    "    load_pretrained=True,\n",
    "    freeze_body=2, \n",
    "    weights_path='./models/yolov3-tiny-model.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.0\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "lines = ['../' + x for x in lines]\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n",
    "with open(test_path) as f:\n",
    "    test_lines = f.readlines()\n",
    "test_lines = ['../' + x for x in test_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-3), \n",
    "              loss={'yolo_loss': lambda y_true, y_pred: y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 383s 5s/step - loss: 400.1338 - val_loss: 75.6389\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 380s 5s/step - loss: 60.0857 - val_loss: 49.7439\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 384s 5s/step - loss: 45.9271 - val_loss: 41.2496\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 380s 5s/step - loss: 39.6830 - val_loss: 36.9034\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 372s 5s/step - loss: 36.3014 - val_loss: 34.2881\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 372s 5s/step - loss: 34.3288 - val_loss: 32.7962\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 367s 5s/step - loss: 32.9036 - val_loss: 31.5132\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 370s 5s/step - loss: 32.0200 - val_loss: 30.7095\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 377s 5s/step - loss: 31.0407 - val_loss: 29.5933\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 372s 5s/step - loss: 30.2366 - val_loss: 29.4556\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 379s 5s/step - loss: 29.9390 - val_loss: 28.4581\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 382s 5s/step - loss: 29.3518 - val_loss: 28.0163\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 378s 5s/step - loss: 28.6465 - val_loss: 27.9776\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 378s 5s/step - loss: 28.7042 - val_loss: 27.5989\n",
      "Epoch 15/50\n",
      "52/78 [===================>..........] - ETA: 41s - loss: 28.8746"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(test_lines, batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, len(test_lines)//batch_size),\n",
    "                epochs=50,\n",
    "                initial_epoch=0,\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_config_sections(config_file):\n",
    "    \"\"\"Convert all config sections to have unique names.\n",
    "    Adds unique suffixes to config sections for compability with configparser.\n",
    "    \"\"\"\n",
    "    section_counters = defaultdict(int)\n",
    "    output_stream = io.StringIO()\n",
    "    with open(config_file) as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('['):\n",
    "                section = line.strip().strip('[]')\n",
    "                _section = section + '_' + str(section_counters[section])\n",
    "                section_counters[section] += 1\n",
    "                line = line.replace(section, _section)\n",
    "            output_stream.write(line)\n",
    "    output_stream.seek(0)\n",
    "    return output_stream\n",
    "\n",
    "def convert():\n",
    "#     config_path = os.path.expanduser(args.config_path)\n",
    "#     weights_path = os.path.expanduser(args.weights_path)\n",
    "#     assert config_path.endswith('.cfg'), '{} is not a .cfg file'.format(\n",
    "#         config_path)\n",
    "#     assert weights_path.endswith(\n",
    "#         '.weights'), '{} is not a .weights file'.format(weights_path)\n",
    "\n",
    "#     output_path = os.path.expanduser(args.output_path)\n",
    "#     assert output_path.endswith(\n",
    "#         '.h5'), 'output path {} is not a .h5 file'.format(output_path)\n",
    "#     output_root = os.path.splitext(output_path)[0]\n",
    "\n",
    "    # Load weights and config.\n",
    "    print('Loading weights.')\n",
    "    weights_file = open(weights_path, 'rb')\n",
    "    major, minor, revision = np.ndarray(\n",
    "        shape=(3, ), dtype='int32', buffer=weights_file.read(12))\n",
    "    if (major*10+minor)>=2 and major<1000 and minor<1000:\n",
    "        seen = np.ndarray(shape=(1,), dtype='int64', buffer=weights_file.read(8))\n",
    "    else:\n",
    "        seen = np.ndarray(shape=(1,), dtype='int32', buffer=weights_file.read(4))\n",
    "    print('Weights Header: ', major, minor, revision, seen)\n",
    "\n",
    "    print('Parsing Darknet config.')\n",
    "    unique_config_file = unique_config_sections(config_path)\n",
    "    cfg_parser = configparser.ConfigParser()\n",
    "    cfg_parser.read_file(unique_config_file)\n",
    "\n",
    "    print('Creating Keras model.')\n",
    "    input_layer = Input(shape=(None, None, 3))\n",
    "    prev_layer = input_layer\n",
    "    all_layers = []\n",
    "\n",
    "    weight_decay = float(cfg_parser['net_0']['decay']\n",
    "                         ) if 'net_0' in cfg_parser.sections() else 5e-4\n",
    "    count = 0\n",
    "    out_index = []\n",
    "    for section in cfg_parser.sections():\n",
    "        print('Parsing section {}'.format(section))\n",
    "        if section.startswith('convolutional'):\n",
    "            filters = int(cfg_parser[section]['filters'])\n",
    "            size = int(cfg_parser[section]['size'])\n",
    "            stride = int(cfg_parser[section]['stride'])\n",
    "            pad = int(cfg_parser[section]['pad'])\n",
    "            activation = cfg_parser[section]['activation']\n",
    "            batch_normalize = 'batch_normalize' in cfg_parser[section]\n",
    "\n",
    "            padding = 'same' if pad == 1 and stride == 1 else 'valid'\n",
    "\n",
    "            # Setting weights.\n",
    "            # Darknet serializes convolutional weights as:\n",
    "            # [bias/beta, [gamma, mean, variance], conv_weights]\n",
    "            prev_layer_shape = K.int_shape(prev_layer)\n",
    "\n",
    "            weights_shape = (size, size, prev_layer_shape[-1], filters)\n",
    "            darknet_w_shape = (filters, weights_shape[2], size, size)\n",
    "            weights_size = np.product(weights_shape)\n",
    "\n",
    "            print('conv2d', 'bn'\n",
    "                  if batch_normalize else '  ', activation, weights_shape)\n",
    "\n",
    "            conv_bias = np.ndarray(\n",
    "                shape=(filters, ),\n",
    "                dtype='float32',\n",
    "                buffer=weights_file.read(filters * 4))\n",
    "            count += filters\n",
    "\n",
    "            if batch_normalize:\n",
    "                bn_weights = np.ndarray(\n",
    "                    shape=(3, filters),\n",
    "                    dtype='float32',\n",
    "                    buffer=weights_file.read(filters * 12))\n",
    "                count += 3 * filters\n",
    "\n",
    "                bn_weight_list = [\n",
    "                    bn_weights[0],  # scale gamma\n",
    "                    conv_bias,  # shift beta\n",
    "                    bn_weights[1],  # running mean\n",
    "                    bn_weights[2]  # running var\n",
    "                ]\n",
    "\n",
    "            conv_weights = np.ndarray(\n",
    "                shape=darknet_w_shape,\n",
    "                dtype='float32',\n",
    "                buffer=weights_file.read(weights_size * 4))\n",
    "            count += weights_size\n",
    "\n",
    "            # DarkNet conv_weights are serialized Caffe-style:\n",
    "            # (out_dim, in_dim, height, width)\n",
    "            # We would like to set these to Tensorflow order:\n",
    "            # (height, width, in_dim, out_dim)\n",
    "            conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "            conv_weights = [conv_weights] if batch_normalize else [\n",
    "                conv_weights, conv_bias\n",
    "            ]\n",
    "\n",
    "            # Handle activation.\n",
    "            act_fn = None\n",
    "            if activation == 'leaky':\n",
    "                pass  # Add advanced activation later.\n",
    "            elif activation != 'linear':\n",
    "                raise ValueError(\n",
    "                    'Unknown activation function `{}` in section {}'.format(\n",
    "                        activation, section))\n",
    "\n",
    "            # Create Conv2D layer\n",
    "            if stride>1:\n",
    "                # Darknet uses left and top padding instead of 'same' mode\n",
    "                prev_layer = ZeroPadding2D(((1,0),(1,0)))(prev_layer)\n",
    "            conv_layer = (Conv2D(\n",
    "                filters, (size, size),\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                use_bias=not batch_normalize,\n",
    "                weights=conv_weights,\n",
    "                activation=act_fn,\n",
    "                padding=padding))(prev_layer)\n",
    "\n",
    "            if batch_normalize:\n",
    "                conv_layer = (BatchNormalization(\n",
    "                    weights=bn_weight_list))(conv_layer)\n",
    "            prev_layer = conv_layer\n",
    "\n",
    "            if activation == 'linear':\n",
    "                all_layers.append(prev_layer)\n",
    "            elif activation == 'leaky':\n",
    "                act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n",
    "                prev_layer = act_layer\n",
    "                all_layers.append(act_layer)\n",
    "\n",
    "        elif section.startswith('route'):\n",
    "            ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]\n",
    "            layers = [all_layers[i] for i in ids]\n",
    "            if len(layers) > 1:\n",
    "                print('Concatenating route layers:', layers)\n",
    "                concatenate_layer = Concatenate()(layers)\n",
    "                all_layers.append(concatenate_layer)\n",
    "                prev_layer = concatenate_layer\n",
    "            else:\n",
    "                skip_layer = layers[0]  # only one layer to route\n",
    "                all_layers.append(skip_layer)\n",
    "                prev_layer = skip_layer\n",
    "\n",
    "        elif section.startswith('maxpool'):\n",
    "            size = int(cfg_parser[section]['size'])\n",
    "            stride = int(cfg_parser[section]['stride'])\n",
    "            all_layers.append(\n",
    "                MaxPooling2D(\n",
    "                    pool_size=(size, size),\n",
    "                    strides=(stride, stride),\n",
    "                    padding='same')(prev_layer))\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('shortcut'):\n",
    "            index = int(cfg_parser[section]['from'])\n",
    "            activation = cfg_parser[section]['activation']\n",
    "            assert activation == 'linear', 'Only linear activation supported.'\n",
    "            all_layers.append(Add()([all_layers[index], prev_layer]))\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('upsample'):\n",
    "            stride = int(cfg_parser[section]['stride'])\n",
    "            assert stride == 2, 'Only stride=2 supported.'\n",
    "            all_layers.append(UpSampling2D(stride)(prev_layer))\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('yolo'):\n",
    "            out_index.append(len(all_layers)-1)\n",
    "            all_layers.append(None)\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('net'):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Unsupported section header type: {}'.format(section))\n",
    "\n",
    "    # Create and save model.\n",
    "    if len(out_index)==0: out_index.append(len(all_layers)-1)\n",
    "    model = Model(inputs=input_layer, outputs=[all_layers[i] for i in out_index])\n",
    "    print(model.summary())\n",
    "#     if args.weights_only:\n",
    "#         model.save_weights('{}'.format(output_path))\n",
    "#         print('Saved Keras weights to {}'.format(output_path))\n",
    "#     else:\n",
    "#         model.save('{}'.format(output_path))\n",
    "#         print('Saved Keras model to {}'.format(output_path))\n",
    "\n",
    "    # Check to see if all weights have been read.\n",
    "    remaining_weights = len(weights_file.read()) / 4\n",
    "    weights_file.close()\n",
    "    print('Read {} of {} from Darknet weights.'.format(count, count +\n",
    "                                                       remaining_weights))\n",
    "    if remaining_weights > 0:\n",
    "        print('Warning: {} unused weights'.format(remaining_weights))\n",
    "\n",
    "#     if args.plot_model:\n",
    "#         plot(model, to_file='{}.png'.format(output_root), show_shapes=True)\n",
    "#         print('Saved model plot to {}.png'.format(output_root))\n",
    "        \n",
    "        \n",
    "#     return model\n",
    "    model.save_weights('{}'.format(output_path))\n",
    "    print('Saved Keras models to {}'.format(output_path))\n",
    "    # Check to see if all weights have been read.\n",
    "    remaining_weights = len(weights_file.read()) / 4\n",
    "    weights_file.close()\n",
    "    print('Read {} of {} from Darknet weights.'.format(count, count +\n",
    "                                                       remaining_weights))\n",
    "    if remaining_weights > 0:\n",
    "        print('Warning: {} unused weights'.format(remaining_weights))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights.\n",
      "Weights Header:  0 2 0 [32013312]\n",
      "Parsing Darknet config.\n",
      "Creating Keras model.\n",
      "Parsing section net_0\n",
      "Parsing section convolutional_0\n",
      "conv2d bn leaky (3, 3, 3, 16)\n",
      "Parsing section maxpool_0\n",
      "Parsing section convolutional_1\n",
      "conv2d bn leaky (3, 3, 16, 32)\n",
      "Parsing section maxpool_1\n",
      "Parsing section convolutional_2\n",
      "conv2d bn leaky (3, 3, 32, 64)\n",
      "Parsing section maxpool_2\n",
      "Parsing section convolutional_3\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section maxpool_3\n",
      "Parsing section convolutional_4\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section maxpool_4\n",
      "Parsing section convolutional_5\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section maxpool_5\n",
      "Parsing section convolutional_6\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_7\n",
      "conv2d bn leaky (1, 1, 1024, 256)\n",
      "Parsing section convolutional_8\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_9\n",
      "conv2d    linear (1, 1, 512, 255)\n",
      "Parsing section yolo_0\n",
      "Parsing section route_0\n",
      "Parsing section convolutional_10\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section upsample_0\n",
      "Parsing section route_1\n",
      "Concatenating route layers: [<tf.Tensor 'up_sampling2d/Identity:0' shape=(None, None, None, 128) dtype=float32>, <tf.Tensor 'leaky_re_lu_4/Identity:0' shape=(None, None, None, 256) dtype=float32>]\n",
      "Parsing section convolutional_11\n",
      "conv2d bn leaky (3, 3, 384, 256)\n",
      "Parsing section convolutional_12\n",
      "conv2d    linear (1, 1, 256, 255)\n",
      "Parsing section yolo_1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 1 432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 1 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, None, None, 1 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 1 0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 4608        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 3 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 6 18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 6 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 73728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 1 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 2 294912      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 2 1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 2 0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 5 1179648     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 5 2048        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 5 0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 4718592     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 1 4096        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 2 262144      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 2 1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 1 512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, None, None, 1 0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 3 0           up_sampling2d[0][0]              \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 5 1179648     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 2 884736      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 5 2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 2 130815      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,858,734\n",
      "Trainable params: 8,852,366\n",
      "Non-trainable params: 6,368\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Read 8858734 of 8858734.0 from Darknet weights.\n",
      "Saved Keras models to ./pretrained/tiny/yolov3.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "read of closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c9392feb288e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./pretrained/tiny/yolov3.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mYOLO3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0f436cbd19ea>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved Keras models to {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# Check to see if all weights have been read.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mremaining_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0mweights_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     print('Read {} of {} from Darknet weights.'.format(count, count +\n",
      "\u001b[0;31mValueError\u001b[0m: read of closed file"
     ]
    }
   ],
   "source": [
    "config_path = './pretrained/tiny/yolov3-tiny.cfg'\n",
    "\n",
    "# config_path = './weights/darknet.cfg'\n",
    "\n",
    "# This was the original file used... progress on validation set was poor or stagnant\n",
    "weights_path = './pretrained/tiny/yolov3-tiny.weights'\n",
    "# weights_path = './weights/yolo.rescore.weights'\n",
    "\n",
    "# Now let's try specifically the tinynet weights\n",
    "# weights_path = './weights/darknet.conv.weights'\n",
    "output_path = './pretrained/tiny/yolov3.hdf5'\n",
    "\n",
    "YOLO3 = convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO3.load_weights(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO3.compile(loss=yolo_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./models/\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1,\n",
    "#                                                 save_best_only=True,\n",
    "#                                                 monitor='val_loss',\n",
    "#                                                 mode='min')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1,\n",
    "                                                 period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO3.predict(mb_train.__getitem__(0)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-be7b18789fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pre2 = YOLO3.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmb_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO3' is not defined"
     ]
    }
   ],
   "source": [
    " pre2 = YOLO3.fit(\n",
    "    x=mb_train,\n",
    "    steps_per_epoch=len(mb_train),\n",
    "    epochs=80,\n",
    "    verbose=1,\n",
    "    validation_data=mb_test,\n",
    "    validation_steps=len(mb_test),\n",
    "     callbacks=[cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-096860afb6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmb_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mb_train.__getitem__(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
