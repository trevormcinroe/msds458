{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 73.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (19.3.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 49.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.22.2-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (3.11.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.14.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.9.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.18.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.12.1)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 18.7 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets) (46.1.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Building wheels for collected packages: dill, promise, future\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=81196 sha256=1410f9585991d12e3efb94656f6bcd3ac342f15b1b42a6c13c16b3263c8449b9\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/49/cf/660924cd9bc5fcddc3a0246fe39800c83028d3ccea244de352\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=23950 sha256=c1853baed1e837d86c50fe87e1f2a25810578f75da2d1397ba0a799a42f3ed32\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=493275 sha256=417445f796c3fe9890afc754b0b7a529d6f3087d3a8fdedf0f958bf4369a52ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "Successfully built dill promise future\n",
      "Installing collected packages: dill, promise, future, googleapis-common-protos, tensorflow-metadata, tqdm, tensorflow-datasets\n",
      "Successfully installed dill-0.3.2 future-0.18.2 googleapis-common-protos-1.52.0 promise-2.3 tensorflow-datasets-3.2.1 tensorflow-metadata-0.22.2 tqdm-4.48.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_anchors(hyper_params):\n",
    "    \"\"\"Generating top left anchors for given anchor_ratios, anchor_scales and image size values.\n",
    "    inputs:\n",
    "        hyper_params = dictionary\n",
    "    outputs:\n",
    "        base_anchors = (anchor_count, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    img_size = hyper_params[\"img_size\"]\n",
    "    anchor_ratios = hyper_params[\"anchor_ratios\"]\n",
    "    anchor_scales = hyper_params[\"anchor_scales\"]\n",
    "    base_anchors = []\n",
    "    for scale in anchor_scales:\n",
    "        scale /= img_size\n",
    "        for ratio in anchor_ratios:\n",
    "            w = tf.sqrt(scale ** 2 / ratio)\n",
    "            h = w * ratio\n",
    "            base_anchors.append([-h / 2, -w / 2, h / 2, w / 2])\n",
    "    return tf.cast(base_anchors, dtype=tf.float32)\n",
    "\n",
    "def generate_anchors(hyper_params):\n",
    "    \"\"\"Broadcasting base_anchors and generating all anchors for given image parameters.\n",
    "    inputs:\n",
    "        hyper_params = dictionary\n",
    "    outputs:\n",
    "        anchors = (output_width * output_height * anchor_count, [y1, x1, y2, x2])\n",
    "            these values in normalized format between [0, 1]\n",
    "    \"\"\"\n",
    "    anchor_count = hyper_params[\"anchor_count\"]\n",
    "    feature_map_shape = hyper_params[\"feature_map_shape\"]\n",
    "    #\n",
    "    stride = 1 / feature_map_shape\n",
    "    grid_coords = tf.cast(tf.range(0, feature_map_shape) / feature_map_shape + stride / 2, dtype=tf.float32)\n",
    "    #\n",
    "    grid_x, grid_y = tf.meshgrid(grid_coords, grid_coords)\n",
    "    flat_grid_x, flat_grid_y = tf.reshape(grid_x, (-1, )), tf.reshape(grid_y, (-1, ))\n",
    "    grid_map = tf.stack([flat_grid_y, flat_grid_x, flat_grid_y, flat_grid_x], axis=-1)\n",
    "    #\n",
    "    base_anchors = generate_base_anchors(hyper_params)\n",
    "    #\n",
    "    anchors = tf.reshape(base_anchors, (1, -1, 4)) + tf.reshape(grid_map, (-1, 1, 4))\n",
    "    anchors = tf.reshape(anchors, (-1, 4))\n",
    "    return tf.clip_by_value(anchors, 0, 1)\n",
    "\n",
    "def non_max_suppression(pred_bboxes, pred_labels, **kwargs):\n",
    "    \"\"\"Applying non maximum suppression.\n",
    "    Details could be found on tensorflow documentation.\n",
    "    https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression\n",
    "    inputs:\n",
    "        pred_bboxes = (batch_size, total_bboxes, total_labels, [y1, x1, y2, x2])\n",
    "            total_labels should be 1 for binary operations like in rpn\n",
    "        pred_labels = (batch_size, total_bboxes, total_labels)\n",
    "        **kwargs = other parameters\n",
    "    outputs:\n",
    "        nms_boxes = (batch_size, max_detections, [y1, x1, y2, x2])\n",
    "        nmsed_scores = (batch_size, max_detections)\n",
    "        nmsed_classes = (batch_size, max_detections)\n",
    "        valid_detections = (batch_size)\n",
    "            Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid.\n",
    "            The rest of the entries are zero paddings.\n",
    "    \"\"\"\n",
    "    return tf.image.combined_non_max_suppression(\n",
    "        pred_bboxes,\n",
    "        pred_labels,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def get_bboxes_from_deltas(anchors, deltas):\n",
    "    \"\"\"Calculating bounding boxes for given bounding box and delta values.\n",
    "    inputs:\n",
    "        anchors = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "        deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n",
    "    outputs:\n",
    "        final_boxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    all_anc_width = anchors[..., 3] - anchors[..., 1]\n",
    "    all_anc_height = anchors[..., 2] - anchors[..., 0]\n",
    "    all_anc_ctr_x = anchors[..., 1] + 0.5 * all_anc_width\n",
    "    all_anc_ctr_y = anchors[..., 0] + 0.5 * all_anc_height\n",
    "    #\n",
    "    all_bbox_width = tf.exp(deltas[..., 3]) * all_anc_width\n",
    "    all_bbox_height = tf.exp(deltas[..., 2]) * all_anc_height\n",
    "    all_bbox_ctr_x = (deltas[..., 1] * all_anc_width) + all_anc_ctr_x\n",
    "    all_bbox_ctr_y = (deltas[..., 0] * all_anc_height) + all_anc_ctr_y\n",
    "    #\n",
    "    y1 = all_bbox_ctr_y - (0.5 * all_bbox_height)\n",
    "    x1 = all_bbox_ctr_x - (0.5 * all_bbox_width)\n",
    "    y2 = all_bbox_height + y1\n",
    "    x2 = all_bbox_width + x1\n",
    "    #\n",
    "    return tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "\n",
    "def get_deltas_from_bboxes(bboxes, gt_boxes):\n",
    "    \"\"\"Calculating bounding box deltas for given bounding box and ground truth boxes.\n",
    "    inputs:\n",
    "        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "        gt_boxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "    outputs:\n",
    "        final_deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n",
    "    \"\"\"\n",
    "    bbox_width = bboxes[..., 3] - bboxes[..., 1]\n",
    "    bbox_height = bboxes[..., 2] - bboxes[..., 0]\n",
    "    bbox_ctr_x = bboxes[..., 1] + 0.5 * bbox_width\n",
    "    bbox_ctr_y = bboxes[..., 0] + 0.5 * bbox_height\n",
    "    #\n",
    "    gt_width = gt_boxes[..., 3] - gt_boxes[..., 1]\n",
    "    gt_height = gt_boxes[..., 2] - gt_boxes[..., 0]\n",
    "    gt_ctr_x = gt_boxes[..., 1] + 0.5 * gt_width\n",
    "    gt_ctr_y = gt_boxes[..., 0] + 0.5 * gt_height\n",
    "    #\n",
    "    bbox_width = tf.where(tf.equal(bbox_width, 0), 1e-3, bbox_width)\n",
    "    bbox_height = tf.where(tf.equal(bbox_height, 0), 1e-3, bbox_height)\n",
    "    delta_x = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.truediv((gt_ctr_x - bbox_ctr_x), bbox_width))\n",
    "    delta_y = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.truediv((gt_ctr_y - bbox_ctr_y), bbox_height))\n",
    "    delta_w = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.math.log(gt_width / bbox_width))\n",
    "    delta_h = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.math.log(gt_height / bbox_height))\n",
    "    #\n",
    "    return tf.stack([delta_y, delta_x, delta_h, delta_w], axis=-1)\n",
    "\n",
    "def generate_iou_map(bboxes, gt_boxes):\n",
    "    \"\"\"Calculating iou values for each ground truth boxes in batched manner.\n",
    "    inputs:\n",
    "        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "        gt_boxes = (batch_size, total_gt_boxes, [y1, x1, y2, x2])\n",
    "    outputs:\n",
    "        iou_map = (batch_size, total_bboxes, total_gt_boxes)\n",
    "    \"\"\"\n",
    "    bbox_y1, bbox_x1, bbox_y2, bbox_x2 = tf.split(bboxes, 4, axis=-1)\n",
    "    gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(gt_boxes, 4, axis=-1)\n",
    "    # Calculate bbox and ground truth boxes areas\n",
    "    gt_area = tf.squeeze((gt_y2 - gt_y1) * (gt_x2 - gt_x1), axis=-1)\n",
    "    bbox_area = tf.squeeze((bbox_y2 - bbox_y1) * (bbox_x2 - bbox_x1), axis=-1)\n",
    "    #\n",
    "    x_top = tf.maximum(bbox_x1, tf.transpose(gt_x1, [0, 2, 1]))\n",
    "    y_top = tf.maximum(bbox_y1, tf.transpose(gt_y1, [0, 2, 1]))\n",
    "    x_bottom = tf.minimum(bbox_x2, tf.transpose(gt_x2, [0, 2, 1]))\n",
    "    y_bottom = tf.minimum(bbox_y2, tf.transpose(gt_y2, [0, 2, 1]))\n",
    "    ### Calculate intersection area\n",
    "    intersection_area = tf.maximum(x_bottom - x_top, 0) * tf.maximum(y_bottom - y_top, 0)\n",
    "    ### Calculate union area\n",
    "    union_area = (tf.expand_dims(bbox_area, -1) + tf.expand_dims(gt_area, 1) - intersection_area)\n",
    "    # Intersection over Union\n",
    "    return intersection_area / union_area\n",
    "\n",
    "def normalize_bboxes(bboxes, height, width):\n",
    "    \"\"\"Normalizing bounding boxes.\n",
    "    inputs:\n",
    "        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "        height = image height\n",
    "        width = image width\n",
    "    outputs:\n",
    "        normalized_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "    \"\"\"\n",
    "    y1 = bboxes[..., 0] / height\n",
    "    x1 = bboxes[..., 1] / width\n",
    "    y2 = bboxes[..., 2] / height\n",
    "    x2 = bboxes[..., 3] / width\n",
    "    return tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "\n",
    "def denormalize_bboxes(bboxes, height, width):\n",
    "    \"\"\"Denormalizing bounding boxes.\n",
    "    inputs:\n",
    "        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "        height = image height\n",
    "        width = image width\n",
    "    outputs:\n",
    "        denormalized_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    y1 = bboxes[..., 0] * height\n",
    "    x1 = bboxes[..., 1] * width\n",
    "    y2 = bboxes[..., 2] * height\n",
    "    x2 = bboxes[..., 3] * width\n",
    "    return tf.round(tf.stack([y1, x1, y2, x2], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image_data, final_height, final_width, apply_augmentation=False, evaluate=False):\n",
    "    \"\"\"Image resizing operation handled before batch operations.\n",
    "    inputs:\n",
    "        image_data = tensorflow dataset image_data\n",
    "        final_height = final image height after resizing\n",
    "        final_width = final image width after resizing\n",
    "    outputs:\n",
    "        img = (final_height, final_width, channels)\n",
    "        gt_boxes = (gt_box_size, [y1, x1, y2, x2])\n",
    "        gt_labels = (gt_box_size)\n",
    "    \"\"\"\n",
    "    img = image_data[\"image\"]\n",
    "    gt_boxes = image_data[\"objects\"][\"bbox\"]\n",
    "    gt_labels = tf.cast(image_data[\"objects\"][\"label\"] + 1, tf.int32)\n",
    "    if evaluate:\n",
    "        not_diff = tf.logical_not(image_data[\"objects\"][\"is_difficult\"])\n",
    "        gt_boxes = gt_boxes[not_diff]\n",
    "        gt_labels = gt_labels[not_diff]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, (final_height, final_width))\n",
    "    if apply_augmentation:\n",
    "        img, gt_boxes = randomly_apply_operation(flip_horizontally, img, gt_boxes)\n",
    "    return img, gt_boxes, gt_labels\n",
    "\n",
    "def get_random_bool():\n",
    "    \"\"\"Generating random boolean.\n",
    "    outputs:\n",
    "        random boolean 0d tensor\n",
    "    \"\"\"\n",
    "    return tf.greater(tf.random.uniform((), dtype=tf.float32), 0.5)\n",
    "\n",
    "def randomly_apply_operation(operation, img, gt_boxes):\n",
    "    \"\"\"Randomly applying given method to image and ground truth boxes.\n",
    "    inputs:\n",
    "        operation = callable method\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "    outputs:\n",
    "        modified_or_not_img = (final_height, final_width, depth)\n",
    "        modified_or_not_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    return tf.cond(\n",
    "        get_random_bool(),\n",
    "        lambda: operation(img, gt_boxes),\n",
    "        lambda: (img, gt_boxes)\n",
    "    )\n",
    "\n",
    "def flip_horizontally(img, gt_boxes):\n",
    "    \"\"\"Flip image horizontally and adjust the ground truth boxes.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "    outputs:\n",
    "        modified_img = (height, width, depth)\n",
    "        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    flipped_img = tf.image.flip_left_right(img)\n",
    "    flipped_gt_boxes = tf.stack([gt_boxes[..., 0],\n",
    "                                1.0 - gt_boxes[..., 3],\n",
    "                                gt_boxes[..., 2],\n",
    "                                1.0 - gt_boxes[..., 1]], -1)\n",
    "    return flipped_img, flipped_gt_boxes\n",
    "\n",
    "def get_dataset(name, split, data_dir=\"~/tensorflow_datasets\"):\n",
    "    \"\"\"Get tensorflow dataset split and info.\n",
    "    inputs:\n",
    "        name = name of the dataset, voc/2007, voc/2012, etc.\n",
    "        split = data split string, should be one of [\"train\", \"validation\", \"test\"]\n",
    "        data_dir = read/write path for tensorflow datasets\n",
    "    outputs:\n",
    "        dataset = tensorflow dataset split\n",
    "        info = tensorflow dataset info\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"train+validation\", \"validation\", \"test\"]\n",
    "    dataset, info = tfds.load(name, split=split, data_dir=data_dir, with_info=True)\n",
    "    return dataset, info\n",
    "\n",
    "def get_total_item_size(info, split):\n",
    "    \"\"\"Get total item size for given split.\n",
    "    inputs:\n",
    "        info = tensorflow dataset info\n",
    "        split = data split string, should be one of [\"train\", \"validation\", \"test\"]\n",
    "    outputs:\n",
    "        total_item_size = number of total items\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"train+validation\", \"validation\", \"test\"]\n",
    "    if split == \"train+validation\":\n",
    "        return info.splits[\"train\"].num_examples + info.splits[\"validation\"].num_examples\n",
    "    return info.splits[split].num_examples\n",
    "\n",
    "def get_labels(info):\n",
    "    \"\"\"Get label names list.\n",
    "    inputs:\n",
    "        info = tensorflow dataset info\n",
    "    outputs:\n",
    "        labels = [labels list]\n",
    "    \"\"\"\n",
    "    return info.features[\"labels\"].names\n",
    "\n",
    "def get_custom_imgs(custom_image_path):\n",
    "    \"\"\"Generating a list of images for given path.\n",
    "    inputs:\n",
    "        custom_image_path = folder of the custom images\n",
    "    outputs:\n",
    "        custom image list = [path1, path2]\n",
    "    \"\"\"\n",
    "    img_paths = []\n",
    "    for path, dir, filenames in os.walk(custom_image_path):\n",
    "        for filename in filenames:\n",
    "            img_paths.append(os.path.join(path, filename))\n",
    "        break\n",
    "    return img_paths\n",
    "\n",
    "def custom_data_generator(img_paths, final_height, final_width):\n",
    "    \"\"\"Yielding custom entities as dataset.\n",
    "    inputs:\n",
    "        img_paths = custom image paths\n",
    "        final_height = final image height after resizing\n",
    "        final_width = final image width after resizing\n",
    "    outputs:\n",
    "        img = (final_height, final_width, depth)\n",
    "        dummy_gt_boxes = (None, None)\n",
    "        dummy_gt_labels = (None, )\n",
    "    \"\"\"\n",
    "    for img_path in img_paths:\n",
    "        image = Image.open(img_path)\n",
    "        resized_image = image.resize((final_width, final_height), Image.LANCZOS)\n",
    "        img = np.array(resized_image)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        yield img, tf.constant([[]], dtype=tf.float32), tf.constant([], dtype=tf.int32)\n",
    "\n",
    "def get_data_types():\n",
    "    \"\"\"Generating data types for tensorflow datasets.\n",
    "    outputs:\n",
    "        data types = output data types for (images, ground truth boxes, ground truth labels)\n",
    "    \"\"\"\n",
    "    return (tf.float32, tf.float32, tf.int32)\n",
    "\n",
    "def get_data_shapes():\n",
    "    \"\"\"Generating data shapes for tensorflow datasets.\n",
    "    outputs:\n",
    "        data shapes = output data shapes for (images, ground truth boxes, ground truth labels)\n",
    "    \"\"\"\n",
    "    return ([None, None, None], [None, None], [None,])\n",
    "\n",
    "def get_padding_values():\n",
    "    \"\"\"Generating padding values for missing values in batch for tensorflow datasets.\n",
    "    outputs:\n",
    "        padding values = padding values with dtypes for (images, ground truth boxes, ground truth labels)\n",
    "    \"\"\"\n",
    "    return (tf.constant(0, tf.float32), tf.constant(0, tf.float32), tf.constant(-1, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stats(labels):\n",
    "    stats = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        stats[i] = {\n",
    "            \"label\": label,\n",
    "            \"total\": 0,\n",
    "            \"tp\": [],\n",
    "            \"fp\": [],\n",
    "            \"scores\": [],\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def update_stats(pred_bboxes, pred_labels, pred_scores, gt_boxes, gt_labels, stats):\n",
    "    iou_map = bbox_utils.generate_iou_map(pred_bboxes, gt_boxes)\n",
    "    merged_iou_map = tf.reduce_max(iou_map, axis=-1)\n",
    "    max_indices_each_gt = tf.argmax(iou_map, axis=-1, output_type=tf.int32)\n",
    "    sorted_ids = tf.argsort(merged_iou_map, direction=\"DESCENDING\")\n",
    "    #\n",
    "    count_holder = tf.unique_with_counts(tf.reshape(gt_labels, (-1,)))\n",
    "    for i, gt_label in enumerate(count_holder[0]):\n",
    "        if gt_label == -1:\n",
    "            continue\n",
    "        gt_label = int(gt_label)\n",
    "        stats[gt_label][\"total\"] += int(count_holder[2][i])\n",
    "    for batch_id, m in enumerate(merged_iou_map):\n",
    "        true_labels = []\n",
    "        for i, sorted_id in enumerate(sorted_ids[batch_id]):\n",
    "            pred_label = pred_labels[batch_id, sorted_id]\n",
    "            if pred_label == 0:\n",
    "                continue\n",
    "            #\n",
    "            iou = merged_iou_map[batch_id, sorted_id]\n",
    "            gt_id = max_indices_each_gt[batch_id, sorted_id]\n",
    "            gt_label = int(gt_labels[batch_id, gt_id])\n",
    "            pred_label = int(pred_label)\n",
    "            score = pred_scores[batch_id, sorted_id]\n",
    "            stats[pred_label][\"scores\"].append(score)\n",
    "            stats[pred_label][\"tp\"].append(0)\n",
    "            stats[pred_label][\"fp\"].append(0)\n",
    "            if iou >= 0.5 and pred_label == gt_label and gt_id not in true_labels:\n",
    "                stats[pred_label][\"tp\"][-1] = 1\n",
    "                true_labels.append(gt_id)\n",
    "            else:\n",
    "                stats[pred_label][\"fp\"][-1] = 1\n",
    "            #\n",
    "        #\n",
    "    #\n",
    "    return stats\n",
    "\n",
    "def calculate_ap(recall, precision):\n",
    "    ap = 0\n",
    "    for r in np.arange(0, 1.1, 0.1):\n",
    "        prec_rec = precision[recall >= r]\n",
    "        if len(prec_rec) > 0:\n",
    "            ap += np.amax(prec_rec)\n",
    "    # By definition AP = sum(max(precision whose recall is above r))/11\n",
    "    ap /= 11\n",
    "    return ap\n",
    "\n",
    "def calculate_mAP(stats):\n",
    "    aps = []\n",
    "    for label in stats:\n",
    "        label_stats = stats[label]\n",
    "        tp = np.array(label_stats[\"tp\"])\n",
    "        fp = np.array(label_stats[\"fp\"])\n",
    "        scores = np.array(label_stats[\"scores\"])\n",
    "        ids = np.argsort(-scores)\n",
    "        total = label_stats[\"total\"]\n",
    "        accumulated_tp = np.cumsum(tp[ids])\n",
    "        accumulated_fp = np.cumsum(fp[ids])\n",
    "        recall = accumulated_tp / total\n",
    "        precision = accumulated_tp / (accumulated_fp + accumulated_tp)\n",
    "        ap = calculate_ap(recall, precision)\n",
    "        stats[label][\"recall\"] = recall\n",
    "        stats[label][\"precision\"] = precision\n",
    "        stats[label][\"AP\"] = ap\n",
    "        aps.append(ap)\n",
    "    mAP = np.mean(aps)\n",
    "    return stats, mAP\n",
    "\n",
    "def evaluate_predictions(dataset, pred_bboxes, pred_labels, pred_scores, labels, batch_size):\n",
    "    stats = init_stats(labels)\n",
    "    for batch_id, image_data in enumerate(dataset):\n",
    "        imgs, gt_boxes, gt_labels = image_data\n",
    "        start = batch_id * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_bboxes, batch_labels, batch_scores = pred_bboxes[start:end], pred_labels[start:end], pred_scores[start:end]\n",
    "        stats = update_stats(batch_bboxes, batch_labels, batch_scores, gt_boxes, gt_labels, stats)\n",
    "    stats, mAP = calculate_mAP(stats)\n",
    "    print(\"mAP: {}\".format(float(mAP)))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_path(model_type, backbone=\"vgg16\", custom_postfix=\"\"):\n",
    "    \"\"\"Generating log path from model_type value for tensorboard.\n",
    "    inputs:\n",
    "        model_type = \"rpn\", \"faster_rcnn\"\n",
    "        backbone = \"vgg16\", \"mobilenet_v2\"\n",
    "        custom_postfix = any custom string for log folder name\n",
    "    outputs:\n",
    "        log_path = tensorboard log path, for example: \"logs/rpn_mobilenet_v2/{date}\"\n",
    "    \"\"\"\n",
    "    return \"logs/{}_{}{}/{}\".format(model_type, backbone, custom_postfix, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "def get_model_path(model_type, backbone=\"vgg16\"):\n",
    "    \"\"\"Generating model path from model_type value for save/load model weights.\n",
    "    inputs:\n",
    "        model_type = \"rpn\", \"faster_rcnn\"\n",
    "        backbone = \"vgg16\", \"mobilenet_v2\"\n",
    "    outputs:\n",
    "        model_path = os model path, for example: \"trained/rpn_vgg16_model_weights.h5\"\n",
    "    \"\"\"\n",
    "    main_path = \"trained\"\n",
    "    if not os.path.exists(main_path):\n",
    "        os.makedirs(main_path)\n",
    "    model_path = os.path.join(main_path, \"{}_{}_model_weights.h5\".format(model_type, backbone))\n",
    "    return model_path\n",
    "\n",
    "# def handle_args():\n",
    "#     \"\"\"Handling of command line arguments using argparse library.\n",
    "#     outputs:\n",
    "#         args = parsed command line arguments\n",
    "#     \"\"\"\n",
    "#     parser = argparse.ArgumentParser(description=\"Faster-RCNN Implementation\")\n",
    "#     parser.add_argument(\"-handle-gpu\", action=\"store_true\", help=\"Tensorflow 2 GPU compatibility flag\")\n",
    "#     parser.add_argument(\"--backbone\", required=False,\n",
    "#                         default=\"mobilenet_v2\",\n",
    "#                         metavar=\"['vgg16', 'mobilenet_v2']\",\n",
    "#                         help=\"Which backbone used for the rpn\")\n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "\n",
    "def is_valid_backbone(backbone):\n",
    "    \"\"\"Handling control of given backbone is valid or not.\n",
    "    inputs:\n",
    "        backbone = given string from command line\n",
    "    \"\"\"\n",
    "    assert backbone in [\"vgg16\", \"mobilenet_v2\"]\n",
    "\n",
    "def handle_gpu_compatibility():\n",
    "    \"\"\"Handling of GPU issues for cuDNN initialize error and memory issues.\"\"\"\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPN = {\n",
    "    \"vgg16\": {\n",
    "        \"img_size\": 500,\n",
    "        \"feature_map_shape\": 31,\n",
    "        \"anchor_ratios\": [1., 2., 1./2.],\n",
    "        \"anchor_scales\": [128, 256, 512],\n",
    "    },\n",
    "    \"mobilenet_v2\": {\n",
    "        \"img_size\": 500,\n",
    "        \"feature_map_shape\": 32,\n",
    "        \"anchor_ratios\": [1., 2., 1./2.],\n",
    "        \"anchor_scales\": [128, 256, 512],\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_hyper_params(backbone, **kwargs):\n",
    "    \"\"\"Generating hyper params in a dynamic way.\n",
    "    inputs:\n",
    "        **kwargs = any value could be updated in the hyper_params\n",
    "    outputs:\n",
    "        hyper_params = dictionary\n",
    "    \"\"\"\n",
    "    hyper_params = RPN[backbone]\n",
    "    hyper_params[\"pre_nms_topn\"] = 6000\n",
    "    hyper_params[\"train_nms_topn\"] = 1500\n",
    "    hyper_params[\"test_nms_topn\"] = 300\n",
    "    hyper_params[\"nms_iou_threshold\"] = 0.7\n",
    "    hyper_params[\"total_pos_bboxes\"] = 128\n",
    "    hyper_params[\"total_neg_bboxes\"] = 128\n",
    "    hyper_params[\"pooling_size\"] = (7, 7)\n",
    "    hyper_params[\"variances\"] = [0.1, 0.1, 0.2, 0.2]\n",
    "    for key, value in kwargs.items():\n",
    "        if key in hyper_params and value:\n",
    "            hyper_params[key] = value\n",
    "    #\n",
    "    hyper_params[\"anchor_count\"] = len(hyper_params[\"anchor_ratios\"]) * len(hyper_params[\"anchor_scales\"])\n",
    "    return hyper_params\n",
    "\n",
    "def get_step_size(total_items, batch_size):\n",
    "    \"\"\"Get step size for given total item size and batch size.\n",
    "    inputs:\n",
    "        total_items = number of total items\n",
    "        batch_size = number of batch size during training or validation\n",
    "    outputs:\n",
    "        step_size = number of step size for model training\n",
    "    \"\"\"\n",
    "    return math.ceil(total_items / batch_size)\n",
    "\n",
    "def randomly_select_xyz_mask(mask, select_xyz):\n",
    "    \"\"\"Selecting x, y, z number of True elements for corresponding batch and replacing others to False\n",
    "    inputs:\n",
    "        mask = (batch_size, [m_bool_value])\n",
    "        select_xyz = ([x_y_z_number_for_corresponding_batch])\n",
    "            example = tf.constant([128, 50, 42], dtype=tf.int32)\n",
    "    outputs:\n",
    "        selected_valid_mask = (batch_size, [m_bool_value])\n",
    "    \"\"\"\n",
    "    maxval = tf.reduce_max(select_xyz) * 10\n",
    "    random_mask = tf.random.uniform(tf.shape(mask), minval=1, maxval=maxval, dtype=tf.int32)\n",
    "    multiplied_mask = tf.cast(mask, tf.int32) * random_mask\n",
    "    sorted_mask = tf.argsort(multiplied_mask, direction=\"DESCENDING\")\n",
    "    sorted_mask_indices = tf.argsort(sorted_mask)\n",
    "    selected_mask = tf.less(sorted_mask_indices, tf.expand_dims(select_xyz, 1))\n",
    "    return tf.logical_and(mask, selected_mask)\n",
    "\n",
    "def faster_rcnn_generator(dataset, anchors, hyper_params):\n",
    "    \"\"\"Tensorflow data generator for fit method, yielding inputs and outputs.\n",
    "    inputs:\n",
    "        dataset = tf.data.Dataset, PaddedBatchDataset\n",
    "        anchors = (total_anchors, [y1, x1, y2, x2])\n",
    "            these values in normalized format between [0, 1]\n",
    "        hyper_params = dictionary\n",
    "    outputs:\n",
    "        yield inputs, outputs\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for image_data in dataset:\n",
    "            img, gt_boxes, gt_labels = image_data\n",
    "            bbox_deltas, bbox_labels = calculate_rpn_actual_outputs(anchors, gt_boxes, gt_labels, hyper_params)\n",
    "            yield (img, gt_boxes, gt_labels, bbox_deltas, bbox_labels), ()\n",
    "\n",
    "def rpn_generator(dataset, anchors, hyper_params):\n",
    "    \"\"\"Tensorflow data generator for fit method, yielding inputs and outputs.\n",
    "    inputs:\n",
    "        dataset = tf.data.Dataset, PaddedBatchDataset\n",
    "        anchors = (total_anchors, [y1, x1, y2, x2])\n",
    "            these values in normalized format between [0, 1]\n",
    "        hyper_params = dictionary\n",
    "    outputs:\n",
    "        yield inputs, outputs\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for image_data in dataset:\n",
    "            img, gt_boxes, gt_labels = image_data\n",
    "            bbox_deltas, bbox_labels = calculate_rpn_actual_outputs(anchors, gt_boxes, gt_labels, hyper_params)\n",
    "            yield img, (bbox_deltas, bbox_labels)\n",
    "\n",
    "def calculate_rpn_actual_outputs(anchors, gt_boxes, gt_labels, hyper_params):\n",
    "    \"\"\"Generating one step data for training or inference.\n",
    "    Batch operations supported.\n",
    "    inputs:\n",
    "        anchors = (total_anchors, [y1, x1, y2, x2])\n",
    "            these values in normalized format between [0, 1]\n",
    "        gt_boxes (batch_size, gt_box_size, [y1, x1, y2, x2])\n",
    "            these values in normalized format between [0, 1]\n",
    "        gt_labels (batch_size, gt_box_size)\n",
    "        hyper_params = dictionary\n",
    "    outputs:\n",
    "        bbox_deltas = (batch_size, total_anchors, [delta_y, delta_x, delta_h, delta_w])\n",
    "        bbox_labels = (batch_size, feature_map_shape, feature_map_shape, anchor_count)\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(gt_boxes)[0]\n",
    "    feature_map_shape = hyper_params[\"feature_map_shape\"]\n",
    "    anchor_count = hyper_params[\"anchor_count\"]\n",
    "    total_pos_bboxes = hyper_params[\"total_pos_bboxes\"]\n",
    "    total_neg_bboxes = hyper_params[\"total_neg_bboxes\"]\n",
    "    variances = hyper_params[\"variances\"]\n",
    "    # Calculate iou values between each bboxes and ground truth boxes\n",
    "    iou_map = bbox_utils.generate_iou_map(anchors, gt_boxes)\n",
    "    # Get max index value for each row\n",
    "    max_indices_each_row = tf.argmax(iou_map, axis=2, output_type=tf.int32)\n",
    "    # Get max index value for each column\n",
    "    max_indices_each_column = tf.argmax(iou_map, axis=1, output_type=tf.int32)\n",
    "    # IoU map has iou values for every gt boxes and we merge these values column wise\n",
    "    merged_iou_map = tf.reduce_max(iou_map, axis=2)\n",
    "    #\n",
    "    pos_mask = tf.greater(merged_iou_map, 0.7)\n",
    "    #\n",
    "    valid_indices_cond = tf.not_equal(gt_labels, -1)\n",
    "    valid_indices = tf.cast(tf.where(valid_indices_cond), tf.int32)\n",
    "    valid_max_indices = max_indices_each_column[valid_indices_cond]\n",
    "    #\n",
    "    scatter_bbox_indices = tf.stack([valid_indices[..., 0], valid_max_indices], 1)\n",
    "    max_pos_mask = tf.scatter_nd(scatter_bbox_indices, tf.fill((tf.shape(valid_indices)[0], ), True), tf.shape(pos_mask))\n",
    "    pos_mask = tf.logical_or(pos_mask, max_pos_mask)\n",
    "    pos_mask = randomly_select_xyz_mask(pos_mask, tf.constant([total_pos_bboxes], dtype=tf.int32))\n",
    "    #\n",
    "    pos_count = tf.reduce_sum(tf.cast(pos_mask, tf.int32), axis=-1)\n",
    "    neg_count = (total_pos_bboxes + total_neg_bboxes) - pos_count\n",
    "    #\n",
    "    neg_mask = tf.logical_and(tf.less(merged_iou_map, 0.3), tf.logical_not(pos_mask))\n",
    "    neg_mask = randomly_select_xyz_mask(neg_mask, neg_count)\n",
    "    #\n",
    "    pos_labels = tf.where(pos_mask, tf.ones_like(pos_mask, dtype=tf.float32), tf.constant(-1.0, dtype=tf.float32))\n",
    "    neg_labels = tf.cast(neg_mask, dtype=tf.float32)\n",
    "    bbox_labels = tf.add(pos_labels, neg_labels)\n",
    "    #\n",
    "    gt_boxes_map = tf.gather(gt_boxes, max_indices_each_row, batch_dims=1)\n",
    "    # Replace negative bboxes with zeros\n",
    "    expanded_gt_boxes = tf.where(tf.expand_dims(pos_mask, -1), gt_boxes_map, tf.zeros_like(gt_boxes_map))\n",
    "    # Calculate delta values between anchors and ground truth bboxes\n",
    "    bbox_deltas = bbox_utils.get_deltas_from_bboxes(anchors, expanded_gt_boxes) / variances\n",
    "    #\n",
    "    # bbox_deltas = tf.reshape(bbox_deltas, (batch_size, feature_map_shape, feature_map_shape, anchor_count * 4))\n",
    "    bbox_labels = tf.reshape(bbox_labels, (batch_size, feature_map_shape, feature_map_shape, anchor_count))\n",
    "    #\n",
    "    return bbox_deltas, bbox_labels\n",
    "\n",
    "def frcnn_cls_loss(*args):\n",
    "    \"\"\"Calculating faster rcnn class loss value.\n",
    "    inputs:\n",
    "        *args = could be (y_true, y_pred) or ((y_true, y_pred), )\n",
    "    outputs:\n",
    "        loss = CategoricalCrossentropy value\n",
    "    \"\"\"\n",
    "    y_true, y_pred = args if len(args) == 2 else args[0]\n",
    "    loss_fn = tf.losses.CategoricalCrossentropy(reduction=tf.losses.Reduction.NONE)\n",
    "    loss_for_all = loss_fn(y_true, y_pred)\n",
    "    #\n",
    "    cond = tf.reduce_any(tf.not_equal(y_true, tf.constant(0.0)), axis=-1)\n",
    "    mask = tf.cast(cond, dtype=tf.float32)\n",
    "    #\n",
    "    conf_loss = tf.reduce_sum(mask * loss_for_all)\n",
    "    total_boxes = tf.maximum(1.0, tf.reduce_sum(mask))\n",
    "    return conf_loss / total_boxes\n",
    "\n",
    "def rpn_cls_loss(*args):\n",
    "    \"\"\"Calculating rpn class loss value.\n",
    "    Rpn actual class value should be 0 or 1.\n",
    "    Because of this we only take into account non -1 values.\n",
    "    inputs:\n",
    "        *args = could be (y_true, y_pred) or ((y_true, y_pred), )\n",
    "    outputs:\n",
    "        loss = BinaryCrossentropy value\n",
    "    \"\"\"\n",
    "    y_true, y_pred = args if len(args) == 2 else args[0]\n",
    "    indices = tf.where(tf.not_equal(y_true, tf.constant(-1.0, dtype=tf.float32)))\n",
    "    target = tf.gather_nd(y_true, indices)\n",
    "    output = tf.gather_nd(y_pred, indices)\n",
    "    lf = tf.losses.BinaryCrossentropy()\n",
    "    return lf(target, output)\n",
    "\n",
    "def reg_loss(*args):\n",
    "    \"\"\"Calculating rpn / faster rcnn regression loss value.\n",
    "    Reg value should be different than zero for actual values.\n",
    "    Because of this we only take into account non zero values.\n",
    "    inputs:\n",
    "        *args = could be (y_true, y_pred) or ((y_true, y_pred), )\n",
    "    outputs:\n",
    "        loss = Huber it's almost the same with the smooth L1 loss\n",
    "    \"\"\"\n",
    "    y_true, y_pred = args if len(args) == 2 else args[0]\n",
    "    y_pred = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1, 4))\n",
    "    #\n",
    "    loss_fn = tf.losses.Huber(reduction=tf.losses.Reduction.NONE)\n",
    "    loss_for_all = loss_fn(y_true, y_pred)\n",
    "    loss_for_all = tf.reduce_sum(loss_for_all, axis=-1)\n",
    "    #\n",
    "    pos_cond = tf.reduce_any(tf.not_equal(y_true, tf.constant(0.0)), axis=-1)\n",
    "    pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n",
    "    #\n",
    "    loc_loss = tf.reduce_sum(pos_mask * loss_for_all)\n",
    "    total_pos_bboxes = tf.maximum(1.0, tf.reduce_sum(pos_mask))\n",
    "    return loc_loss / total_pos_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 50\n",
    "load_weights = False\n",
    "with_voc_2012 = True\n",
    "backbone = args.backbone\n",
    "io_utils.is_valid_backbone(backbone)\n",
    "\n",
    "if backbone == \"mobilenet_v2\":\n",
    "    from models.rpn_mobilenet_v2 import get_model as get_rpn_model\n",
    "else:\n",
    "    from models.rpn_vgg16 import get_model as get_rpn_model\n",
    "\n",
    "hyper_params = train_utils.get_hyper_params(backbone)\n",
    "\n",
    "train_data, dataset_info = data_utils.get_dataset(\"voc/2007\", \"train+validation\")\n",
    "val_data, _ = data_utils.get_dataset(\"voc/2007\", \"test\")\n",
    "train_total_items = data_utils.get_total_item_size(dataset_info, \"train+validation\")\n",
    "val_total_items = data_utils.get_total_item_size(dataset_info, \"test\")\n",
    "\n",
    "if with_voc_2012:\n",
    "    voc_2012_data, voc_2012_info = data_utils.get_dataset(\"voc/2012\", \"train+validation\")\n",
    "    voc_2012_total_items = data_utils.get_total_item_size(voc_2012_info, \"train+validation\")\n",
    "    train_total_items += voc_2012_total_items\n",
    "    train_data = train_data.concatenate(voc_2012_data)\n",
    "\n",
    "labels = data_utils.get_labels(dataset_info)\n",
    "# We add 1 class for background\n",
    "hyper_params[\"total_labels\"] = len(labels) + 1\n",
    "#\n",
    "img_size = hyper_params[\"img_size\"]\n",
    "train_data = train_data.map(lambda x : data_utils.preprocessing(x, img_size, img_size, apply_augmentation=True))\n",
    "val_data = val_data.map(lambda x : data_utils.preprocessing(x, img_size, img_size))\n",
    "\n",
    "data_shapes = data_utils.get_data_shapes()\n",
    "padding_values = data_utils.get_padding_values()\n",
    "train_data = train_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)\n",
    "val_data = val_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)\n",
    "\n",
    "anchors = bbox_utils.generate_anchors(hyper_params)\n",
    "frcnn_train_feed = train_utils.faster_rcnn_generator(train_data, anchors, hyper_params)\n",
    "frcnn_val_feed = train_utils.faster_rcnn_generator(val_data, anchors, hyper_params)\n",
    "#\n",
    "rpn_model, feature_extractor = get_rpn_model(hyper_params)\n",
    "frcnn_model = faster_rcnn.get_model(feature_extractor, rpn_model, anchors, hyper_params)\n",
    "frcnn_model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-5),\n",
    "                    loss=[None] * len(frcnn_model.output))\n",
    "faster_rcnn.init_model(frcnn_model, hyper_params)\n",
    "# If you have pretrained rpn model\n",
    "# You can load rpn weights for faster training\n",
    "rpn_load_weights = False\n",
    "if rpn_load_weights:\n",
    "    rpn_model_path = io_utils.get_model_path(\"rpn\", backbone)\n",
    "    rpn_model.load_weights(rpn_model_path)\n",
    "# Load weights\n",
    "frcnn_model_path = io_utils.get_model_path(\"faster_rcnn\", backbone)\n",
    "\n",
    "if load_weights:\n",
    "    frcnn_model.load_weights(frcnn_model_path)\n",
    "log_path = io_utils.get_log_path(\"faster_rcnn\", backbone)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(frcnn_model_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "tensorboard_callback = TensorBoard(log_dir=log_path)\n",
    "\n",
    "step_size_train = train_utils.get_step_size(train_total_items, batch_size)\n",
    "step_size_val = train_utils.get_step_size(val_total_items, batch_size)\n",
    "frcnn_model.fit(frcnn_train_feed,\n",
    "                steps_per_epoch=step_size_train,\n",
    "                validation_data=frcnn_val_feed,\n",
    "                validation_steps=step_size_val,\n",
    "                epochs=epochs,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "evaluate = False\n",
    "use_custom_images = False\n",
    "custom_image_path = \"data/images/\"\n",
    "backbone = args.backbone\n",
    "io_utils.is_valid_backbone(backbone)\n",
    "\n",
    "if backbone == \"mobilenet_v2\":\n",
    "    from models.rpn_mobilenet_v2 import get_model as get_rpn_model\n",
    "else:\n",
    "    from models.rpn_vgg16 import get_model as get_rpn_model\n",
    "\n",
    "hyper_params = train_utils.get_hyper_params(backbone)\n",
    "\n",
    "test_data, dataset_info = data_utils.get_dataset(\"voc/2007\", \"test\")\n",
    "total_items = data_utils.get_total_item_size(dataset_info, \"test\")\n",
    "labels = data_utils.get_labels(dataset_info)\n",
    "labels = [\"bg\"] + labels\n",
    "hyper_params[\"total_labels\"] = len(labels)\n",
    "img_size = hyper_params[\"img_size\"]\n",
    "\n",
    "data_types = data_utils.get_data_types()\n",
    "data_shapes = data_utils.get_data_shapes()\n",
    "padding_values = data_utils.get_padding_values()\n",
    "\n",
    "if use_custom_images:\n",
    "    img_paths = data_utils.get_custom_imgs(custom_image_path)\n",
    "    total_items = len(img_paths)\n",
    "    test_data = tf.data.Dataset.from_generator(lambda: data_utils.custom_data_generator(\n",
    "                                               img_paths, img_size, img_size), data_types, data_shapes)\n",
    "else:\n",
    "    test_data = test_data.map(lambda x : data_utils.preprocessing(x, img_size, img_size, evaluate=evaluate))\n",
    "#\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)\n",
    "#\n",
    "anchors = bbox_utils.generate_anchors(hyper_params)\n",
    "rpn_model, feature_extractor = get_rpn_model(hyper_params)\n",
    "frcnn_model = faster_rcnn.get_model(feature_extractor, rpn_model, anchors, hyper_params, mode=\"inference\")\n",
    "#\n",
    "frcnn_model_path = io_utils.get_model_path(\"faster_rcnn\", backbone)\n",
    "frcnn_model.load_weights(frcnn_model_path)\n",
    "\n",
    "step_size = train_utils.get_step_size(total_items, batch_size)\n",
    "pred_bboxes, pred_labels, pred_scores = frcnn_model.predict(test_data, steps=step_size, verbose=1)\n",
    "\n",
    "if evaluate:\n",
    "    eval_utils.evaluate_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)\n",
    "else:\n",
    "    drawing_utils.draw_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
