{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/semeval_train_ko.csv')\n",
    "df_train_norm = pd.read_csv('./data/semeval_train.csv')\n",
    "df_test = pd.read_csv('./data/semeval_test_ko.csv')\n",
    "\n",
    "# df_test = df_test.iloc[:len(df_test)-1, :]\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm['sent_1'] = df_train_norm['sentence_A']\n",
    "df_train_norm['sent_2'] = df_train_norm['sentence_B']\n",
    "df_train_norm['sim'] = df_train_norm['relatedness_score']\n",
    "df_train_norm = df_train_norm[['sent_1', 'sent_2', 'sim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_norm = pd.read_csv('./data/semeval_test.csv')\n",
    "df_test_norm['sim'] = norm(df_test_norm['relatedness_score'])\n",
    "df_test_norm['sent_1'] =df_test_norm['sentence_A']\n",
    "df_test_norm['sent_2'] = df_test_norm['sentence_B']\n",
    "df_test_norm = df_test_norm[['sent_1', 'sent_2', 'sim']]\n",
    "df_test_norm.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sent_1'] = df_test['sentence_A']\n",
    "df_test['sent_2'] = df_test['sentence_B']\n",
    "\n",
    "# df_train['sent_1'] = df_train['sentence_A']\n",
    "# df_train['sent_2'] = df_train['sentence_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.append(df_train_norm)\n",
    "df_test = df_test.append(df_test_norm)\n",
    "\n",
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(model, datasets, question_cols):\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']\n",
    "    questions_cols = question_cols\n",
    "\n",
    "    # Iterate over the questions only of both training and test datasets\n",
    "    for dataset in datasets:\n",
    "        for index, row in dataset.iterrows():\n",
    "\n",
    "            # Iterate through the text of both questions of the row\n",
    "            for question in questions_cols:\n",
    "\n",
    "                q2n = []  # q2n -> question numbers representation\n",
    "                for word in text_to_word_list(row[question]):\n",
    "\n",
    "                    # # Check for unwanted words\n",
    "                    if word not in model.vocab:\n",
    "                        continue\n",
    "\n",
    "                    if word not in vocabulary:\n",
    "                        vocabulary[word] = len(inverse_vocabulary)\n",
    "                        q2n.append(len(inverse_vocabulary))\n",
    "                        inverse_vocabulary.append(word)\n",
    "                    else:\n",
    "                        q2n.append(vocabulary[word])\n",
    "\n",
    "                # Replace questions as word to question as number representationindex, question, q2n\n",
    "                dataset.at[index, question]= q2n\n",
    "\n",
    "    embedding_dim = model.vector_size\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabulary.items():\n",
    "        if word in model.vocab:\n",
    "            embeddings[index] = model.word_vec(word)\n",
    "\n",
    "    return embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/enwiki_20180420_300d.txt')\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, embedding_dim = prepare_embeddings(model=word2vec, datasets=[df_train, df_test], question_cols=['sent_1', 'sent_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(df_train.sent_1.map(lambda x: len(x)).max(),\n",
    "                     df_train.sent_2.map(lambda x: len(x)).max(),\n",
    "                     df_test.sent_1.map(lambda x: len(x)).max(),\n",
    "                     df_test.sent_2.map(lambda x: len(x)).max())\n",
    "\n",
    "X_train = {'left': df_train.sent_1, 'right': df_train.sent_2}\n",
    "X_test = {'left': df_test.sent_1, 'right': df_test.sent_2}\n",
    "\n",
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "        dataset[side] = tf.keras.preprocessing.sequence.pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\" Helper function for the similarity estimate of the RNNs outputs\"\"\"\n",
    "    return tf.keras.backend.exp(-tf.keras.backend.sum(tf.keras.backend.abs(left - right), \n",
    "                                                      axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "left_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(len(embeddings), \n",
    "                                            embedding_dim, \n",
    "                                            weights=[embeddings], \n",
    "                                            input_length=max_seq_length,\n",
    "                                            trainable=False)\n",
    "\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same RNN\n",
    "# shared_gru = tf.keras.layers.GRU(100, name='gru', recurrent_activation='sigmoid', reset_after=True,\n",
    "#                                 bias_initializer=tf.keras.initializers.Constant(2.5), dropout=0.0,\n",
    "#                                 kernel_regularizer=None, recurrent_dropout=0.0)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same RNN\n",
    "shared_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(100, name='gru', recurrent_activation='sigmoid', reset_after=True,\n",
    "                                                               bias_initializer=tf.keras.initializers.Constant(2.5), dropout=0.0,\n",
    "                                                               kernel_regularizer=None, recurrent_dropout=0.0))\n",
    "\n",
    "\n",
    "shared_dense = tf.keras.layers.Dense(50, activation='relu')\n",
    "dp = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "left_output = shared_gru(encoded_left)\n",
    "right_output = shared_gru(encoded_right)\n",
    "\n",
    "# left_output_den = shared_dense(left_output)\n",
    "# right_output_den = shared_dense(right_output)\n",
    "\n",
    "# left_output_dp = dp(left_output_den)\n",
    "# right_output_dp = dp(right_output_den)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "magru_distance = tf.keras.layers.Lambda(\n",
    "    function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "    output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "magru = tf.keras.Model([left_input, right_input], [magru_distance])\n",
    "optimizer=tf.keras.optimizers.Adadelta(learning_rate=1, rho=0.985, clipvalue=2.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipvalue=1.5)\n",
    "\n",
    "magru.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = magru.fit([X_train['left'], X_train['right']], \n",
    "                np.array(df_train['sim']), \n",
    "                epochs=500, \n",
    "                batch_size=64,\n",
    "                validation_data=([X_test['left'], X_test['right']], df_test['sim'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = magru.predict([X_train['left'], X_train['right']])\n",
    "print(f\"Train: {pearsonr([x[0] for x in preds.tolist()], df_train['sim'])[0]}\")\n",
    "preds = magru.predict([X_test['left'], X_test['right']])\n",
    "print(f\"Test: {pearsonr([x[0] for x in preds.tolist()], df_test['sim'])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are using TTA, please use the following...\n",
    "b_idx = 0\n",
    "e_idx = 4927\n",
    "\n",
    "p_avg = list()\n",
    "while e_idx < len(preds):\n",
    "    first = preds[b_idx]\n",
    "    second = preds[e_idx]\n",
    "    p_avg.append(np.mean([first, second]))\n",
    "    b_idx += 1\n",
    "    e_idx += 1\n",
    "    \n",
    "pearsonr(p_avg, df_test_norm['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
